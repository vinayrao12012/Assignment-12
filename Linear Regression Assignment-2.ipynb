{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb76ccf2",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ecc4c2",
   "metadata": {},
   "source": [
    "R-squared, also known as the coefficient of determination, is a statistical metric used to evaluate the goodness of fit of a linear regression model. It measures the proportion of the variance in the dependent variable that is explained by the independent variables in the model. R-squared is a value between 0 and 1, with higher values indicating a better fit of the model to the data.\n",
    "\n",
    "- R^2 = 1 - SSR / SST\n",
    "- SSR = Sum of squares residual\n",
    "- SST = Sum of squares total\n",
    "\n",
    "1. SSR (Sum of Squared Residuals) is the sum of the squared differences between the actual values of the dependent variable and the predicted values by the regression model.\n",
    "2. SST (Total Sum of Squares) is the sum of the squared differences between the actual values of the dependent variable and the mean of the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9339d91f",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060809c0",
   "metadata": {},
   "source": [
    "Adjusted R-squared is a modified version of the regular R-squared (coefficient of determination) that takes into account the number of predictors (independent variables) in a linear regression model. While regular R-squared measures the proportion of variance in the dependent variable explained by all predictors, adjusted R-squared adjusts this value based on the number of predictors used.\n",
    "\n",
    "\n",
    "Key Differences:\n",
    "\n",
    "1. Regular R-squared measures the goodness of fit but doesn't consider the number of predictors, which can lead to misleading interpretations in models with many predictors.\n",
    "2. Adjusted R-squared adjusts the regular R-squared value based on the number of predictors and the sample size, providing a more balanced assessment of model fit and helping to avoid overfitting\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6c77ec",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5d8be7",
   "metadata": {},
   "source": [
    "Adjusted R-squared is more appropriate to use in situations where you are comparing and evaluating multiple linear regression models with different numbers of predictors (independent variables). It is particularly useful when you want to strike a balance between model complexity and goodness of fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c1dfe9",
   "metadata": {},
   "source": [
    "## Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa1e014",
   "metadata": {},
   "source": [
    "In the context of regression analysis, RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are common evaluation metrics used to assess the performance of a regression model by measuring the accuracy of its predictions. \n",
    "\n",
    "1. Root Mean Squared Error(RMSE):\n",
    "- Calculation: RMSE is calculated by taking the square root of the mean of the squared differences between the predicted values (Y) and the actual values (yi) for each data point:\n",
    "  sqrt(Summation(yi - Y)^2 / n)\n",
    "  \n",
    "- Interpretation: RMSE measures the square root of the average squared error between the model's predictions and the actual values. It is expressed in the same units as the dependent variable (Y), making it easy to interpret. Lower RMSE values indicate better model performance, with a value of 0 indicating a perfect fit.  \n",
    "\n",
    "\n",
    "2. Mean Squared Error(MSE):\n",
    "- Calculation: MSE is calculated as the mean of the squared differences between the predicted values (Y) and the actual values (yi) for each data point:\n",
    "  Summation(yi - Y)^2 / n\n",
    "- Interpretation: MSE measures the average squared error between the model's predictions and the actual values. Like RMSE, lower MSE values indicate better model performance. MSE is sensitive to outliers because it squares the errors, giving higher weight to larger errors.\n",
    "\n",
    "\n",
    "3. Mean Absolute Error(MAE):\n",
    "- Calculation: MAE is calculated as the mean of the absolute differences between the predicted values (Y) and the actual values (yi) for each data point:\n",
    "  Summation abs(y - Y) / n\n",
    "- Interpretation: MAE measures the average absolute error between the model's predictions and the actual values. It is less sensitive to outliers than MSE because it does not square the errors. Like RMSE and MSE, lower MAE values indicate better model performance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5ea3b1",
   "metadata": {},
   "source": [
    "## Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060e3f01",
   "metadata": {},
   "source": [
    "1. Mean Squared Error(MSE):\n",
    "- Advantages:\n",
    "    1. It is differentiable\n",
    "    2. It has one local and one global minima\n",
    "- Disadvantages:\n",
    "    1. Not Robust to outliers\n",
    "    2. It is not in the same units\n",
    "    \n",
    "2. Mean Absolute Error(MAE):\n",
    "- Advantages:\n",
    "    1. It is robust to outliers.\n",
    "    2. It will be in the same units\n",
    "- Disadvantages:\n",
    "    1. Convergence usually takes time optimization is complex\n",
    "    2. Time consuming\n",
    "    \n",
    "3. Root Mean Squared Error(RMSE):\n",
    "- Advantages:\n",
    "    1. It is in the same units \n",
    "    2. It is differentiable\n",
    "- Disadvantages: \n",
    "    1. It is not robust to outliers\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3543b64",
   "metadata": {},
   "source": [
    "## Q6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb32fd8",
   "metadata": {},
   "source": [
    "Lasso regularization, also known as L1 regularization, is a technique used in machine learning and linear regression to prevent overfitting by adding a penalty term to the linear regression cost function. It encourages the model to shrink the coefficients of less important predictors toward zero, effectively performing feature selection. \n",
    "\n",
    "Lasso regularization differs from Ridge regularization (L2 regularization) in the type of penalty it imposes on the coefficients:\n",
    "\n",
    "- Lasso (L1 Regularization): Lasso adds a penalty term based on the absolute values of the coefficients. It encourages some coefficients to become exactly zero, effectively performing feature selection. This means that Lasso can lead to sparse models with only a subset of the predictors included.\n",
    "\n",
    "- Ridge (L2 Regularization): Ridge adds a penalty term based on the square of the coefficients. It encourages all coefficients to be small but does not force them to be exactly zero. Ridge can be effective at reducing multicollinearity (high correlation between predictors) and controlling the magnitude of coefficients.\n",
    "\n",
    "\n",
    "\n",
    "Feature Selection: When you suspect that many predictors are irrelevant or redundant and want to automatically select a subset of the most important predictors for better model interpretability and simplicity.\n",
    "\n",
    "Sparse Models: When you prefer models with fewer predictors, making it easier to interpret and potentially reducing computational complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73619701",
   "metadata": {},
   "source": [
    "## Q7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fb7eee",
   "metadata": {},
   "source": [
    "Regularized linear models help prevent overfitting in machine learning by adding a penalty term to the cost function that encourages the model to have smaller and more constrained coefficients (slopes). This regularization term discourages the model from fitting the training data too closely, which can lead to overfitting.\n",
    "\n",
    "Examples:\n",
    "Consider a simple linear regression problem where you want to predict a person's salary based on their years of education. Without regularization, you might fit a high-degree polynomial to the data, producing a model that closely fits the training data points, including the noise.\n",
    "\n",
    "With regularization (e.g., Ridge regularization), the model is encouraged to have smaller coefficients, which results in a simpler linear relationship between education and salary. This helps prevent the model from fitting the noise and overfitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e277db20",
   "metadata": {},
   "source": [
    "## Q8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa89be72",
   "metadata": {},
   "source": [
    "Regularized linear models are powerful tools for regression analysis, but they have certain limitations and may not always be the best choice for every regression problem:\n",
    "\n",
    "1. Linearity Assumption: Regularized linear models, including Ridge and Lasso regression, assume a linear relationship between predictors and the target variable. If the true relationship in the data is non-linear, using these models may lead to underfitting and poor predictive performance.\n",
    "\n",
    "2. Data Size: In very small datasets, regularization may not be as effective because there may not be enough data to estimate the regularization penalty accurately. In such cases, simpler linear models without regularization may be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28d7e5b",
   "metadata": {},
   "source": [
    "## Q9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46076391",
   "metadata": {},
   "source": [
    "The choice between Model A (RMSE of 10) and Model B (MAE of 8) as the better performer depends on the specific context and the priorities of the problem you are trying to solve. Each metric (RMSE and MAE) has its own strengths and limitations, and the choice should align with your goals and preferences.\n",
    "\n",
    "Comparing RMSE (Root Mean Squared Error) and MAE (Mean Absolute Error):\n",
    "\n",
    "- RMSE: RMSE gives higher weight to larger errors because it squares the differences between predicted and actual values. It is sensitive to outliers and penalizes models more for large errors. In this case, a RMSE of 10 suggests that, on average, the model's predictions deviate by about 10 units from the actual values.\n",
    "\n",
    "- MAE: MAE, on the other hand, treats all errors equally and does not give higher weight to larger errors. It is less sensitive to outliers. An MAE of 8 indicates that, on average, the model's predictions deviate by about 8 units from the actual values\n",
    "\n",
    "Considerations for Choosing a Metric:\n",
    "\n",
    "- Outliers: If your dataset contains outliers or large errors that you consider important to penalize heavily, RMSE may be more appropriate. RMSE gives greater weight to these outliers due to the squaring operation.\n",
    "\n",
    "- Robustness: If you want a metric that is more robust to outliers and treats all errors equally, MAE may be preferred. MAE is less influenced by extreme values.\n",
    "\n",
    "\n",
    "Limitations of the Choice of Metric:\n",
    "\n",
    "- Scale Sensitivity: Both RMSE and MAE are sensitive to the scale of the dependent variable. If the scale of the variable changes, the absolute values of these metrics will change accordingly, making direct comparisons between models with different scales challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efbb569",
   "metadata": {},
   "source": [
    "## Q10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857ea0e0",
   "metadata": {},
   "source": [
    "The choice between Ridge regularization (Model A) and Lasso regularization (Model B) with specific regularization parameters (0.1 for Ridge and 0.5 for Lasso) depends on the characteristics of the data and the goals of the modeling task. Both regularization methods have their strengths and limitations, and the choice should align with the specific context.\n",
    "\n",
    "Choosing Between Ridge and Lasso:\n",
    "\n",
    "The choice between Ridge and Lasso regularization depends on your priorities and the characteristics of your data:\n",
    "\n",
    "- If You Want a Simpler Model: If you prefer a simpler, more interpretable model with fewer predictors (feature selection), Lasso regularization (Model B) may be the better choice. It sets some coefficients to zero, effectively excluding those predictors from the model.\n",
    "- If You Want to Control Collinearity: If your dataset has high multicollinearity among predictors, Ridge regularization (Model A) may be more appropriate. It can help control multicollinearity by shrinking correlated coefficients.\n",
    "\n",
    "Trade-offs and Limitations:\n",
    "\n",
    "- Regularization Parameter Selection: The effectiveness of Ridge and Lasso regularization depends on choosing an appropriate value for the regularization parameter (λ). The choice of λ can impact model performance, and tuning it may require experimentation.\n",
    "\n",
    "- Feature Selection Trade-off: While Lasso performs feature selection, this can be a limitation if important predictors are incorrectly excluded from the model. Careful consideration of the business or domain context is essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540dddcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
