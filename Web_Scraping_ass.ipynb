{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc05575f-0d80-4d06-9d6c-e2d8477caef4",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80213553-0b4c-40cc-ab67-3e3c4d295961",
   "metadata": {},
   "source": [
    "Web scraping refers to the process of automatically extracting information and data from websites. It involves using software tools or scripts to navigate web pages, retrieve content, and extract relevant data for various purposes. Web scraping allows you to gather data from websites without manual copying and pasting, making it efficient for collecting large amounts of data quickly.\n",
    "\n",
    "\n",
    "\n",
    "It is primarily used for extracting data from the web page.\n",
    "\n",
    "\n",
    "Three areas where web scraping is used.\n",
    "\n",
    "\n",
    "1. Job Market Analysis: Web scraping can help researchers analyze job listings, salary trends, and skill requirements in the job market, providing insights for job seekers and employers.\n",
    "\n",
    "2. Weather Data Collection: Weather forecasting services scrape data from various sources to provide accurate and up-to-date weather predictions.\n",
    "\n",
    "3. Social Media Monitoring: Companies use web scraping to monitor social media platforms for mentions, comments, and sentiment related to their brand, products, or services.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f8dad6-5734-43b9-a06c-c7416b74f8cb",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46308564-1101-4434-8649-b91cb80db40d",
   "metadata": {},
   "source": [
    "Web scraping can be performed using various methods and techniques, each with its own advantages and limitations. Here are some common methods used for web scraping:\n",
    "\n",
    "1. Manual Copy-Pasting: This is the most basic method where data is manually copied and pasted from web pages into a document or spreadsheet. While simple, it's only suitable for small amounts of data and is not efficient for large-scale scraping.\n",
    "\n",
    "\n",
    "2. Regular Expressions : Regular expressions can be used to search and extract specific patterns of text from web pages. This method requires a good understanding of regular expressions and might not handle complex HTML structures well.\n",
    "\n",
    "3. DOM  scraping : Parsing the Document Object Model (DOM) of a web page allows you to navigate the HTML structure and extract desired elements using programming languages like JavaScript (in the browser) or libraries like Beautiful Soup (Python) for server-side scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ecde3c-4744-453d-b2d2-0bbc2497a8fe",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813532c3-e198-4000-94ed-15c849ae9077",
   "metadata": {},
   "source": [
    "Beautiful Soup is a Python library that is widely used for web scraping purposes. It provides tools for parsing HTML and XML documents, navigating the document's structure, and extracting data from it. Beautiful Soup makes it easier to work with complex HTML structures and extract specific information without having to manually parse the raw HTML code.\n",
    "\n",
    "It is used: \n",
    "\n",
    "1. HTML Parsing: Beautiful Soup is designed to parse HTML and XML documents. It handles poorly formatted or broken HTML gracefully, making it a useful tool for scraping data from websites with varying levels of HTML quality.\n",
    "\n",
    "2. Searching and Filtering: Beautiful Soup provides methods to search for elements based on various criteria, such as tag names, attributes, text content, and more. This makes it efficient to locate and extract specific data points from a web page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759c548f-fbe5-4227-8bdc-b1e4ff624845",
   "metadata": {},
   "source": [
    "## Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42594486-fad2-4995-8aef-b3150681bf27",
   "metadata": {},
   "source": [
    "Flask is a Python web framework that is commonly used to build web applications and APIs. It might not be a direct tool for web scraping itself, but it can be used alongside web scraping projects for various reasons:\n",
    "\n",
    "1. Creating a Web Interface: Flask can be used to create a web interface that allows users to interact with your web scraping functionality. You can build a user-friendly front-end where users can input URLs, specify parameters, or select options for scraping, and then display the results in a structured way.\n",
    "\n",
    "2. API Endpoints: If you want to expose your web scraping functionality as an API, Flask is a great choice.\n",
    "\n",
    "3. Data Visualization: Flask can be used to visualize the scraped data using various charting libraries. You can display the insights gained from web scraping in an interactive and visually appealing manner on a web page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29342e08-10d7-41a1-a645-836984933ae0",
   "metadata": {},
   "source": [
    "## Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953013fb-cf35-4b20-82cb-6ddba35e533b",
   "metadata": {},
   "source": [
    "The names of aws services used in this project is:\n",
    " \n",
    "1. Code Pipeline\n",
    "2. Elastic Bean stak\n",
    "\n",
    "Code pipiline use:\n",
    "\n",
    "1. Automated Deployment: When your scraping code undergoes changes or updates, CodePipeline can automate the process of building, testing, and deploying these changes to your web application or backend.\n",
    "\n",
    "2. Code Testing: CodePipeline can integrate with testing frameworks to ensure that your scraping code works as expected before deploying it. Automated testing helps catch issues early in the development process.\n",
    "\n",
    "\n",
    "Elastic Pipeline use:\n",
    "\n",
    "1. Application Hosting: You can deploy your web scraping application using Elastic Beanstalk. It automatically handles the deployment, capacity provisioning, load balancing, and scaling of the application.\n",
    "\n",
    "2. Easy Deployment: You can easily deploy your scraping application using the Elastic Beanstalk command-line interface or directly from your source code repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb0c959-0385-43e0-ac1a-db272aeff1a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
