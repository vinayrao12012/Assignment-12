{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "927a32ff",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95b83ee",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as min-max normalization or feature scaling, is a data preprocessing technique used to transform numerical features in a dataset so that they have a specific range of values, typically between 0 and 1. This scaling method is particularly useful when you want to standardize the range of variables with different units or scales.\n",
    "\n",
    "This scaling method is particularly useful when you want to standardize the range of variables with different units or scales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80764d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0a9db4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup</th>\n",
       "      <th>dropoff</th>\n",
       "      <th>passengers</th>\n",
       "      <th>distance</th>\n",
       "      <th>fare</th>\n",
       "      <th>tip</th>\n",
       "      <th>tolls</th>\n",
       "      <th>total</th>\n",
       "      <th>color</th>\n",
       "      <th>payment</th>\n",
       "      <th>pickup_zone</th>\n",
       "      <th>dropoff_zone</th>\n",
       "      <th>pickup_borough</th>\n",
       "      <th>dropoff_borough</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-03-23 20:21:09</td>\n",
       "      <td>2019-03-23 20:27:24</td>\n",
       "      <td>1</td>\n",
       "      <td>1.60</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.95</td>\n",
       "      <td>yellow</td>\n",
       "      <td>credit card</td>\n",
       "      <td>Lenox Hill West</td>\n",
       "      <td>UN/Turtle Bay South</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-03-04 16:11:55</td>\n",
       "      <td>2019-03-04 16:19:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.79</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.30</td>\n",
       "      <td>yellow</td>\n",
       "      <td>cash</td>\n",
       "      <td>Upper West Side South</td>\n",
       "      <td>Upper West Side South</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-03-27 17:53:01</td>\n",
       "      <td>2019-03-27 18:00:25</td>\n",
       "      <td>1</td>\n",
       "      <td>1.37</td>\n",
       "      <td>7.5</td>\n",
       "      <td>2.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.16</td>\n",
       "      <td>yellow</td>\n",
       "      <td>credit card</td>\n",
       "      <td>Alphabet City</td>\n",
       "      <td>West Village</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-03-10 01:23:59</td>\n",
       "      <td>2019-03-10 01:49:51</td>\n",
       "      <td>1</td>\n",
       "      <td>7.70</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.95</td>\n",
       "      <td>yellow</td>\n",
       "      <td>credit card</td>\n",
       "      <td>Hudson Sq</td>\n",
       "      <td>Yorkville West</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-03-30 13:27:42</td>\n",
       "      <td>2019-03-30 13:37:14</td>\n",
       "      <td>3</td>\n",
       "      <td>2.16</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.40</td>\n",
       "      <td>yellow</td>\n",
       "      <td>credit card</td>\n",
       "      <td>Midtown East</td>\n",
       "      <td>Yorkville West</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               pickup             dropoff  passengers  distance  fare   tip  \\\n",
       "0 2019-03-23 20:21:09 2019-03-23 20:27:24           1      1.60   7.0  2.15   \n",
       "1 2019-03-04 16:11:55 2019-03-04 16:19:00           1      0.79   5.0  0.00   \n",
       "2 2019-03-27 17:53:01 2019-03-27 18:00:25           1      1.37   7.5  2.36   \n",
       "3 2019-03-10 01:23:59 2019-03-10 01:49:51           1      7.70  27.0  6.15   \n",
       "4 2019-03-30 13:27:42 2019-03-30 13:37:14           3      2.16   9.0  1.10   \n",
       "\n",
       "   tolls  total   color      payment            pickup_zone  \\\n",
       "0    0.0  12.95  yellow  credit card        Lenox Hill West   \n",
       "1    0.0   9.30  yellow         cash  Upper West Side South   \n",
       "2    0.0  14.16  yellow  credit card          Alphabet City   \n",
       "3    0.0  36.95  yellow  credit card              Hudson Sq   \n",
       "4    0.0  13.40  yellow  credit card           Midtown East   \n",
       "\n",
       "            dropoff_zone pickup_borough dropoff_borough  \n",
       "0    UN/Turtle Bay South      Manhattan       Manhattan  \n",
       "1  Upper West Side South      Manhattan       Manhattan  \n",
       "2           West Village      Manhattan       Manhattan  \n",
       "3         Yorkville West      Manhattan       Manhattan  \n",
       "4         Yorkville West      Manhattan       Manhattan  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sns.load_dataset('taxis')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c208c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8821ef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad319be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04359673, 0.04026846, 0.06475904],\n",
       "       [0.02152589, 0.02684564, 0.        ],\n",
       "       [0.0373297 , 0.04362416, 0.07108434],\n",
       "       ...,\n",
       "       [0.11280654, 0.10067114, 0.        ],\n",
       "       [0.03051771, 0.03355705, 0.        ],\n",
       "       [0.10490463, 0.09395973, 0.10120482]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max.fit_transform(df[[\"distance\",\"fare\",\"tip\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a31109",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2f9a22",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as Normalization, is a feature scaling method used in data preprocessing to transform numerical features in a dataset so that they have a unit norm or length of 1.\n",
    "\n",
    "Differ:\n",
    "\n",
    "Normalization vs. Range Transformation:\n",
    "\n",
    "1. Unit Vector Scaling: It normalizes the feature vectors so that they have a unit norm, meaning their length becomes 1. This technique focuses on preserving the direction of feature vectors.\n",
    "2. Min-Max Scaling: It transforms feature values to a specific range, typically [0, 1], but you can choose a different range. Min-Max scaling adjusts the magnitude of feature values while potentially changing the direction of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c914aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "707fb802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.213461</td>\n",
       "      <td>0.933894</td>\n",
       "      <td>0.286839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.156064</td>\n",
       "      <td>0.987747</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.171657</td>\n",
       "      <td>0.939731</td>\n",
       "      <td>0.295702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.267899</td>\n",
       "      <td>0.939386</td>\n",
       "      <td>0.213971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.231742</td>\n",
       "      <td>0.965592</td>\n",
       "      <td>0.118017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6428</th>\n",
       "      <td>0.160133</td>\n",
       "      <td>0.960800</td>\n",
       "      <td>0.226322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6429</th>\n",
       "      <td>0.307453</td>\n",
       "      <td>0.951563</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6430</th>\n",
       "      <td>0.250500</td>\n",
       "      <td>0.968117</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6431</th>\n",
       "      <td>0.183497</td>\n",
       "      <td>0.983020</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6432</th>\n",
       "      <td>0.242956</td>\n",
       "      <td>0.946580</td>\n",
       "      <td>0.212034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6433 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2\n",
       "0     0.213461  0.933894  0.286839\n",
       "1     0.156064  0.987747  0.000000\n",
       "2     0.171657  0.939731  0.295702\n",
       "3     0.267899  0.939386  0.213971\n",
       "4     0.231742  0.965592  0.118017\n",
       "...        ...       ...       ...\n",
       "6428  0.160133  0.960800  0.226322\n",
       "6429  0.307453  0.951563  0.000000\n",
       "6430  0.250500  0.968117  0.000000\n",
       "6431  0.183497  0.983020  0.000000\n",
       "6432  0.242956  0.946580  0.212034\n",
       "\n",
       "[6433 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(normalize(df[[\"distance\",\"fare\",\"tip\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7695685b",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf65876",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used in statistics and machine learning to simplify complex datasets while preserving as much of the important information as possible.\n",
    "\n",
    "\n",
    "1. Centering the Data\n",
    "2. Covariance Matrix Computation\n",
    "3. Eigenvalue and Eigenvector Calculation\n",
    "4. Selecting Principal Components\n",
    "5. Dimensionality Reduction\n",
    "\n",
    "Suppose you have a dataset with three features: \"Height,\" \"Weight,\" and \"Age,\" and you want to reduce the dimensionality while preserving as much information as possible.Suppose you have a dataset with three features: \"Height,\" \"Weight,\" and \"Age,\" and you want to reduce the dimensionality while preserving as much information as possible.\n",
    "\n",
    "1. Centering the Data:\n",
    "    Subtract the mean of each feature from all data points.\n",
    "2. Covariance Matrix Computation:\n",
    "    Compute the covariance matrix to understand the relationships between the features.\n",
    "3. Eigenvalue and Eigenvector Calculation:\n",
    "    Calculate the eigenvalues and eigenvectors of the covariance matrix.  \n",
    "4. Selecting Principal Components:\n",
    "    Sort the eigenvalues in descending order and choose how many of them to keep. Let's say you decide to keep the first two principal components.  \n",
    "5. Dimensionality Reduction:\n",
    "    Project the original data onto the subspace defined by the first two principal components.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029aab57",
   "metadata": {},
   "source": [
    "## Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe970c3",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) and Feature Extraction are closely related concepts in the context of dimensionality reduction and data analysis. PCA can be used as a technique for feature extraction, allowing you to transform the original features into a new set of features (principal components) that capture the most important information in the data while reducing its dimensionality.\n",
    "\n",
    "\n",
    "1. PCA extracts new features (principal components) from the original features in a dataset. These principal components are linear combinations of the original features.\n",
    "2. The principal components are selected in such a way that they capture the maximum variance in the data, effectively summarizing the key patterns and relationships among the original features.\n",
    "3. By selecting a subset of the principal components, you can create a reduced-dimensional representation of the data, effectively performing feature extraction.\n",
    "\n",
    "Examples:\n",
    "\n",
    "1. Data Preprocessing:\n",
    "    Begin by preprocessing the data, which may include handling missing values, encoding categorical variables, and standardizing the numerical features.\n",
    "2. Applying PCA:\n",
    "    Apply PCA to the preprocessed data. PCA will calculate the principal components that capture the maximum variance in the dataset. \n",
    "3. Reduced-Dimensional Representation:\n",
    "    The first three principal components become the new extracted features. These components are linear combinations of the original 10 features.\n",
    "4. Dimensionality Reduction:\n",
    "    1. The original dataset had 10 features, but the reduced-dimensional representation has only 3 features (the three selected principal components).\n",
    "    2. This dimensionality reduction simplifies the data while preserving its essential information.    \n",
    "5. Modeling and Analysis:\n",
    "    You can now use this reduced-dimensional dataset for various tasks, such as building predictive models or performing data analysis.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e634a1b6",
   "metadata": {},
   "source": [
    "## Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55291e09",
   "metadata": {},
   "source": [
    "1. Data Understanding and Preprocessing:\n",
    "    Start by understanding the dataset and performing initial data preprocessing steps, such as handling missing values and encoding categorical features if any.\n",
    "2. Select Numerical Features:\n",
    "    Identify the numerical features that you want to scale. In your case, these might include \"Price,\" \"Rating,\" and \"Delivery Time.\"  \n",
    "3. Min-Max Scaling:\n",
    "    For each numerical feature, apply Min-Max scaling using the following formula: \n",
    "        X(scaled) = (X - mean) / (X(max) - X(min))\n",
    "        \n",
    "4. Scaling Features:\n",
    "    For each numerical feature, calculate the minimum (X min) and maximum (X max) values present in the dataset.   \n",
    "5. Apply Min-Max Scaling\n",
    "    For each numerical feature, apply the Min-Max scaling formula to transform the feature values to the [0, 1] range\n",
    "    \n",
    "6. Interpretation:\n",
    "    1.After Min-Max Scaling the numerical featureswill be scaled to the range [0,1]\n",
    "    A value of 0 indicates the minimum observed value for that feature, while a value of 1 indicates the maximum observed value. Values in between represent scaled versions of the original data.\n",
    "    \n",
    "7. Model Building:\n",
    "    Use the preprocessed and scaled data as input for building your recommendation system model. The scaled features will ensure that the different numerical features contribute equally to the recommendation process.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4a9695",
   "metadata": {},
   "source": [
    "## Q6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9957afc",
   "metadata": {},
   "source": [
    "1. Data Preprocessing:\n",
    "    1. Begin by preprocessing the dataset, which may include handling missing values, encoding categorical variables, and standardizing or normalizing the numerical features. Ensure that the data is clean and ready for analysis.\n",
    "    \n",
    "2. Feature Selection or Extraction:\n",
    "\n",
    "    1. Assess the relevance and importance of each feature in the dataset. Given that you have financial data and market trends, it's likely that some features are highly correlated or redundant.\n",
    "    2. Decide whether you want to perform feature selection, feature extraction, or a combination of both. PCA falls under the category of feature extraction because it creates new features (principal components) from the original ones.\n",
    "    \n",
    "3. Applying PCA:\n",
    "\n",
    "    1. Select the numerical features from your dataset that you want to include in the PCA analysis. These features can include various financial metrics, market indicators, and any other relevant data.\n",
    "    2. Standardize or normalize the selected features to ensure that they have similar scales. PCA is sensitive to the scale of features, so standardization is often recommended.  \n",
    "    \n",
    "4. PCA Calculation:\n",
    "    1. Calculate the principal components of the selected features using PCA. The principal components are linear combinations of the original features that capture the most significant variation in the data.  \n",
    "    \n",
    "5. Selecting the Number of Principal Components:\n",
    "    1. Determine how many principal components you want to keep. This decision can be based on the explained variance. You can choose to retain a sufficient number of components to capture a certain percentage of the total variance (e.g., 95% of the variance).    \n",
    "    \n",
    "6. Dimensionality Reduction:\n",
    "    1. Project the original dataset onto the subspace defined by the selected principal components. This reduces the dimensionality of your dataset while preserving the most important information.    \n",
    "    \n",
    "7. Model Building:\n",
    "    1. Use the reduced-dimensional dataset, consisting of the selected principal components, as input for building your stock price prediction model. This can include regression models, time series forecasting models, or machine learning algorithms. \n",
    "    \n",
    "8. Evaluation and Fine-Tuning:\n",
    "    1. Evaluate the performance of your stock price prediction model using appropriate evaluation metrics. You may need to fine-tune the model's hyperparameters or iterate through the process to achieve the best results.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57873d78",
   "metadata": {},
   "source": [
    "## Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3746884",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [1, 5, 10, 15, 20]\n",
    "min_lst = min(lst)\n",
    "max_lst = max(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f8221ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = []\n",
    "for i in lst:\n",
    "    scale = (i - min_lst) / (max_lst - min_lst)\n",
    "    normalized_data.append(scale)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "113302e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.21052631578947367, 0.47368421052631576, 0.7368421052631579, 1.0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7352846",
   "metadata": {},
   "source": [
    "## Q8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25a6929",
   "metadata": {},
   "source": [
    "Performing feature extraction using PCA (Principal Component Analysis) involves transforming the original features into a set of uncorrelated principal components while reducing dimensionality.\n",
    "\n",
    "1. Data Preprocessing:\n",
    "    1. Start by preprocessing the data, which may include standardization or normalization of numerical features and encoding categorical features (like gender) into numerical values if needed.\n",
    "    \n",
    "2. PCA Calculation:\n",
    "    1. Apply PCA to the dataset. PCA will compute a set of principal components that capture the maximum variance in the data.\n",
    "    2. The principal components are linear combinations of the original features. \n",
    "    \n",
    "3. Determine Explained Variance:\n",
    "    1. After applying PCA, you'll obtain a list of principal components along with their associated eigenvalues. The eigenvalues represent the variance explained by each principal component.\n",
    "    \n",
    "4. Choose the Number of Principal Components:\n",
    "    1. The decision of how many principal components to retain depends on your desired level of dimensionality reduction and how much variance you are willing to sacrifice for simplicity.  \n",
    "    \n",
    "5. Reasoning for the Number of Components:\n",
    "    1. In practice, the number of principal components retained is often a balance between dimensionality reduction and information retention. Retaining too many components may not provide significant benefits and could lead to overfitting in modeling.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba9b0ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
